{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7f34e7",
   "metadata": {},
   "source": [
    "> # Introduction\n",
    "\n",
    "Object detection task has become mature with excellent performance. Detecting a person and a cup of coffee is now a relatively easy task. However, what if the person is **holding** the cup of coffee? Not only localizing and classifying those objects, we also have to understand the **interaction** between the objects. I believe one of the crucial missions of computer vision is to **understand** visual perceptions.\n",
    "\n",
    "**Human-Object Interaction (HOI)** detection is a task to identify \"a set of interactions\" in an image. Two main tasks of HOI are\n",
    "\n",
    "1. **localization** of the subject of interaction such as humans\n",
    "\n",
    "2. **Classification** of the interaction labels\n",
    "\n",
    "Hence, the according output is $<human, object, interaction>$ triplet.\n",
    "\n",
    "> # Previous Works\n",
    "\n",
    "Some of previous works treat HOI in a **sequential** manner where object detection is performed first, followed by associating every pair of the detected objects with a subsequent network. This is very time-consuming and requires high computation cost.\n",
    "\n",
    "Other methods suggest **parallel** HOI detectors for faster inference time. These works directly localize interactions with union boxes or interaction points, thus replacing the subsequent neural network in the sequential detector. However, they predict interaction based on a simple heuristic such as distance or IoUs, yielding suboptimal performance due to hand-crafted post-processing step which requires manual threshold search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfeada",
   "metadata": {},
   "source": [
    "> # HOTR: **H**uman-**O**bject interaction **TR**ansformer\n",
    "\n",
    "| ![space-1.jpg](../images/hotr1.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* |\n",
    "\n",
    "To overcome the previous drawbacks, a fast and accurate HOI model, **HOTR**, was proposed which predicts human-object interactions **at once with a direct set prediction**. HOTR utilizes **transformer encoder-decoder** architecture to predict HOI triplets. This architecture overcomes the previous limitations. \n",
    "\n",
    "First, the end-to-end training step gets rid of the time-consuming hand-crafted post-processing stage. Also, the inherent nature of **self-attention** mechanisms of transformers extracts **contextual relationships** between the sub ject and the target of interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a447d",
   "metadata": {},
   "source": [
    "> # HOI Detection as Set Prediction\n",
    "\n",
    "Similar to object detection, HOTR treats object detection task as a **set prediction problem** that predicts **localization of a human region**, an **object region**, and **multi-label classification of the interaction types**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3aaa53",
   "metadata": {},
   "source": [
    "> # HOTR Architecture\n",
    "\n",
    "HOTR utilizes a **transformer encoder-decoder** architecture with a **shared encoder** and **two parallel decoders**L instance decoder and interaction decoder. Then, the proposed **HO pointer** associates the outputs of the two decoders to produce the final HOI triplets.\n",
    "\n",
    "## Transformer Encoder-Decoder architecture\n",
    "\n",
    "Similar to DETR, the backbone **CNN** and a **shared encoder** extracts the **global context**.\n",
    "\n",
    "Then, **two sets of positional embeddings**, **instance queries** and the **interaction quries**, are fed into the **two parallel decoders**: **instance decoder** and the **interaction decoder**. Details are shown in the figure above.\n",
    "\n",
    "The **instance decoder** accepts the instance queries and transforms them to **instance representations** for object detection.\n",
    "\n",
    "The **interaction decoder** accepts the interaction queries and transforms them to **interaction representations** for interaction detection.\n",
    "\n",
    "Then, the **feed-forward networks (FFNs)** are applied to the **interaction representation**, obtaining a **Human Pointer**, an **Object Pointer**, and **interaction type**. An interesting part of this paper is that the **interaction representation** (not the instance representation!) \"localizes\" humand and object regions by **pointing the corresponding instance representations** using the Human Pointer and Object Pointer (**HO Pointer**). This approach avoids direct bounding box regression which incurs a problem when an object(a cup of coffee) involves in multiple interactions(multiple humans or different behaviors such as holding or pushing). \n",
    "\n",
    "In simple words, **HO Pointer** \"associates\" instance and interaction representations. This approach enables more efficient learning of localization without needing to learn localization redundantly for every interaction.\n",
    "\n",
    "Now, let's look at the heart of HOTR: **HO Pointers**\n",
    "\n",
    "## HO Pointers\n",
    "\n",
    "| ![space-1.jpg](../images/hotr2.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* |>\n",
    "\n",
    "As mentioned earlier, feed-forward networks are applied to the interaction representation and obtain a **Human Pointer**, an **Object Pointer**, and **interaction type**. **HO Pointers** (i.e., Human and Object Pointer) contain the **indices** of the corresponding **instance representations** of the interacting human and object. Let's take a deeper look at a more formal description.\n",
    "\n",
    "[1] The interaction decoder transforms $K$ interaction queries to $K$ interaction representations\n",
    "\n",
    "| ![space-1.jpg](../images/hotr3.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* |\n",
    "\n",
    "[2] Then, each **interaction representation** $z_i$ is fed into **two feed-forward networks**:\n",
    "\n",
    "$$FFN_h : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$$\n",
    "\n",
    "$$FFN_o : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$$\n",
    "\n",
    "to obtain vectors $v_i^h$ and $v_i^o$\n",
    "\n",
    "$$v_i^h = FFN_h(z_i)$$\n",
    "\n",
    "$$v_i^o = FFN_o(z_i)$$\n",
    "\n",
    "| ![space-1.jpg](../images/hotr4.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* |\n",
    "\n",
    "[3] Finally, the **Human Pointer** and **Object Pointer** $\\hat{c}_i^h$ and $\\hat{c}_i^o$ which are the **indices** of the **instance representations** with the **highest similarity scores** are obtained by\n",
    "\n",
    "$$\\hat{c}_i^h = \\underset{j}{argmax}(sim(v_i^h, \\mu_j))$$\n",
    "\n",
    "$$\\hat{c}_i^o = \\underset{j}{argmax}(sim(v_i^o, \\mu_j))$$\n",
    "\n",
    "where $\\mu_j$ is the $j-th$ **instance representation** and $sim(u,v) = \\frac{u^Tv}{\\lVert u \\rVert \\lVert v \\rVert}$\n",
    "\n",
    "| ![space-1.jpg](../images/hotr5.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* |\n",
    "\n",
    "## Recomposition for HOI Set Prediction\n",
    "\n",
    "| ![space-1.jpg](../images/hotr6.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* | \n",
    "\n",
    "We've now obtained the following:\n",
    "\n",
    "1. $N$ instance presentations $\\mu$\n",
    "\n",
    "2. $K$ interaction representations $z$ and their HO Pointers $\\hat{c}^h$ and $\\hat{c}^o$\n",
    "\n",
    "With $\\gamma$ interaction classes, we **recompose** by applying the **feed-forward networks** for **bounding box regression** and **action classification** as\n",
    "\n",
    "$$FFN_{box}: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\rightarrow \\mathbb{R}^4$$\n",
    "\n",
    "$$FFN_{act}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{\\gamma}$$\n",
    "\n",
    "Then, the **final HOI prediction** for the $i-th$ interaction representation $z_i$ is\n",
    "\n",
    "$$\\hat{b}_i^h = FFN_{box}(\\mu_{\\hat{c}_i^h}) \\in \\mathbb{R}^4$$\n",
    "\n",
    "$$\\hat{b}_i^o = FFN_{box}(\\mu_{\\hat{c}_i^o}) \\in \\mathbb{R}^4$$\n",
    "\n",
    "$$\\hat{a}_i = FFN_{act}(z_i) \\in \\mathbb{R}^{\\gamma}$$\n",
    "\n",
    "Hence, we now obtained the set of $K$ **HOI triplets**:\n",
    "\n",
    "$$\\text{HOI triplets}= \\{ \\langle \\hat{b}_i^h, \\hat{b}_i^o, \\hat{a}_i \\rangle \\}_{i=1}^K$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278acac",
   "metadata": {},
   "source": [
    "> # Complexity & Inference time\n",
    "\n",
    "Previous parallel HOI detectors reduce inference time with a fast matching of triplets based on simple heuristics such as distances or IoU.\n",
    "\n",
    "HOTR further reduces the inference time (after object detection) by **associating** $K$ interactions with $N$ instances. Hence, the time complexity is $o(KN)$.\n",
    "\n",
    "Also, HOTR doesn't require the expensive post-processing steps such as NMS for the interaction region and triplet matching. HOTR reduces the inference time by $4 \\sim 8ms$ but still shows performance gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15ee44",
   "metadata": {},
   "source": [
    "> # Hungarian Matching for HOI Detection\n",
    "\n",
    "HOTR predicts $K$ HOI triplets: human box, object box, and **binary classification** for $a$ types of interactions. Thus, each prediction captures a **unique** $\\langle human, object \\rangle$ pair with **one or more** interactions.\n",
    "\n",
    "$K$ is set to be larger than the intuitive number of interacting pairs in an image.\n",
    "\n",
    "Let $\\mathcal{Y}$ denote the set of **GT HOI tripltes** and $\\hat{\\mathcal{Y}}=\\{ \\hat{y}_i \\}_{i=1}^K$ as the set of $K$ **predictions**.\n",
    "\n",
    "We consider $\\mathcal{Y}$ as a set of size $K$ with padding of $\\emptyset$ for no interaction as $K$ is larger than the number of unique interacting pairs.\n",
    "\n",
    "Then, we search for a permutation of $K$ elements $\\sigma \\in \\mathfrak{S}_K$ to find a bipartiate matching with the **lowest cost** :\n",
    "\n",
    "$$\\hat{\\sigma} = \\underset{\\sigma \\in \\mathfrak{S}_K}{argmin} \\sum_i^K C_{match}(y_i, \\hat{y}_{\\sigma(i)})$$\n",
    "\n",
    "where $C_{match}$ is a **pair-wise matching cose** between GT $y_i$ and a prediction with index $\\sigma(i)$.\n",
    "\n",
    "But the ground truth label $y_i$ is in the form of $\\langle hbox, obox, action \\rangle$ while $\\hat{y}_{\\sigma(i)}$ is in the form of $\\langle hidx, oidx, action \\rangle$ while $h$ and $o$ denote human and object. Thus, we need to modify the cost function to correctly compute the cost.\n",
    "\n",
    "Let $\\Phi$ be a **mapping function** from GT label $\\langle hidx, oidx \\rangle$ to GT label $\\langle hbox, obox \\rangle$ by optimal assignment for object detection\n",
    "\n",
    "$$\\Phi: idx \\rightarrow box$$\n",
    "\n",
    "By inversing the mapping function,\n",
    "\n",
    "$$\\Phi^{-1}: box \\rightarrow idx$$\n",
    "\n",
    "we get the GT **idx** from the GT **box**.\n",
    "\n",
    "Then, let $M \\in \\mathbb{R}^{d \\times N}$ be a set of **normalized instance representations** $\\mu' = \\frac{\\mu}{\\lVert \\mu \\rVert} \\in \\mathbb{R}^d$, i.e., $M = [\\mu'_1 \\cdots \\mu'_N]$.\n",
    "\n",
    "We compute $\\hat{P}^h \\in \\mathbb{R}^{K \\times N}$ which is the set of **softmax predictions** for the **H Pointer** (represent the **index** for instance representation):\n",
    "\n",
    "$$\\hat{P}^h = \\Vert_{i=1}^K softmax((\\bar{v_i}^h)^T M)$$\n",
    "\n",
    "where $\\Vert_{i=1}^K$ denotes the **vertical stack** of the row vectors and $\\bar{v}_i^h = v_i^ / \\lVert v_i^h \\rVert$. $\\hat{P}^o$ is defined simiarly.\n",
    "\n",
    "Now, given the GT $y_i = (b_i^h, b_i^o, a_i)$, $\\hat{P}^h$ and $\\hat{P}^o$, we **convert the GT box** to **indices** by\n",
    "\n",
    "$$c_i^h = \\Phi^{-1}(b_i^h)$$\n",
    "\n",
    "$$c_i^o = \\Phi^{-1}(b_i^o)$$\n",
    "\n",
    "and compute the **matching cost function**:\n",
    "\n",
    "$$C_{match}(y_i, \\hat{y}\\_{\\sigma(i)}) = -\\alpha \\cdot \\mathbb{1}_{\\{a_i \\neq \\emptyset \\}} \\hat{P}^h[\\sigma(i), c_i^h]$$\n",
    "\n",
    "$$-\\beta \\cdot \\mathbb{1}\\_{\\{a_i \\neq \\emptyset \\}} \\hat{P}^o[\\sigma(i), c_i^o]$$\n",
    "\n",
    "$$+\\mathbb{1}\\_{\\{a_i \\neq \\emptyset \\}} \\mathcal{L}\\_{act}[a_i, \\hat{a}\\_{\\sigma(i)}]$$\n",
    "\n",
    "where $\\hat{P}[i,j]$ denotes the element at $i-th$ row and $j-th$ column, and $\\hat{a}\\_{\\sigma(i)}$ is the predicted action.\n",
    "\n",
    "The **action matching cost** is computed as\n",
    "\n",
    "$$\\mathcal{L}\\_{act}(a_i, \\hat{a}\\_{\\sigma(i)}) = BCELoss(a_i,\\hat{a}\\_{\\sigma(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b95b0e",
   "metadata": {},
   "source": [
    "> # Final Set Prediction Loss for HOTR\n",
    "\n",
    "Now we compute the **Hungarian loss** for **all pairs** matched above. \n",
    "\n",
    "The loss for the HOI triplets involve the **localization loss** and **classification loss**\n",
    "\n",
    "$$\\mathcal{L}_H = \\sum_{i=1}^K[\\mathcal{L}_{loc}(c_i^h, c_i^o, z_{\\sigma(i)}) + \\mathcal{L}_{act}(a_i, \\hat{a}_{\\sigma(i)})]$$\n",
    "\n",
    "where the localization loss $\\mathcal{L}_{loc}(c_i^h, c_i^o, z_{\\sigma(i)})$ is denoted as\n",
    "\n",
    "$$\\mathcal{L}_{loc} = -\\log \\frac{exp(sim(FFN_h(z_{\\sigma(i)}), \\mu_{c_i^h} ) / \\tau)}{\\sum_{k=1}^N exp(sim(FFN_h(z_{\\sigma(i)}), \\mu_k ) / \\tau)} -\\log \\frac{exp(sim(FFN_o(z_{\\sigma(i)}), \\mu_{c_i^o} ) / \\tau)}{\\sum_{k=1}^N exp(sim(FFN_o(z_{\\sigma(i)}), \\mu_k ) / \\tau)}$$\n",
    "\n",
    "where $\\tau$ is the temperature controling the smoothness of the loss function. $\\tau=0.1$ is used in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b58893",
   "metadata": {},
   "source": [
    "> # Defining No-Interaction with HOTR\n",
    "\n",
    "HOI utilizes **multi-label classification** for action classification and each action is treated as an **individual binary classification**. Therefore, there's no **explicit no-interaction label** like in DETR which could suppress the probability of other classes. Hence, HOTR sets an explicit class that learns the **interactiveness** (1 if there's any interaction between the pair, 0 otherwise) and suppresses the predictions for reundant pairs with a **low interactiveness score**. The low interactiveness score is defined as **No-Interaction class**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a581eb3",
   "metadata": {},
   "source": [
    "> # Experiments\n",
    "\n",
    "HOTR was tested on the **V-COCO** and **HICO-DET** dataset. Below are the performance details\n",
    "\n",
    "| ![space-1.jpg](../images/hotr7.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* | \n",
    "\n",
    "| ![space-1.jpg](../images/hotr8.png) | \n",
    "|:--:| \n",
    "| *[BiSeNet](https://arxiv.org/abs/1808.00897)* | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7cb1f",
   "metadata": {},
   "source": [
    "> # Conclusion\n",
    "\n",
    "Understanding visual perceptions is one of the most crucial task in computer vision. Human-Object Interaction (HOI) detection is a more advanced task than plain object detection task, understanding **interactions** between the subject of the interaction (human) and the objects in the scene. HOTR introduces a novel transformer encoder-decoder architecture which greatly reduce inference time with various methods including HO Pointer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf881a5",
   "metadata": {},
   "source": [
    "> # References\n",
    "\n",
    "[1] https://arxiv.org/abs/2104.13682\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=hmfRkkmt0WE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
